[
["index.html", "RAP Companion Preface 0.1 Discovery 0.2 Getting the balance right 0.3 Inspiration", " RAP Companion Matthew Gregory, Matthew Upson 2017-07-17 Preface Producing official statistics for publications is a key function of many teams across Government. It’s a time consuming and meticulous process to ensure that statistics are accurate and timely. With open source software becoming more widely used, there’s now a range of tools and techniques that can be used to reduce production time, whilst maintaining and even improving the quality of the publications. This book is about these techniques: what they are, and how we can use them. 0.1 Discovery Something is better than nothing. This is NOT TRUE in the agile world; consider the opportunity cost1. Prior to commiting to RAP development in your team ask yourself two questions: Is there a genuine user need for this new product/service? Does another product/service exist that you can re-use or adapt?2 0.2 Getting the balance right Whilst incredibly powerful, these approaches should not be seen as panacea for all the difficulties of statistics production: however, implementing even a few of these techniques can drive benefits in auditability, speed, quality, and knowledge transfer. There is a balance to be struck between ease of maintenance and the level of automation: this is likely to differ for every publication or team. These techniques are however tried and tested for software development and most already feature in the Service Manual: in this project we have just applied these methodologies to a new area. 0.3 Inspiration This book was inspired by, and draws heavily from a Government Digital Service blog post by Dr. Matthew Upson, itself taking much inspiration from the fields of DevOps and reproducible research. the loss of other alternatives when one alternative is chosen↩ There’s probably some code you can adapt on Github, such as the eesectors pacakge↩ "],
["intro.html", "Chapter 1 Introduction 1.1 Objectives", " Chapter 1 Introduction The Reproducible Analytical Pipeline (RAP) is an alternative production methodology for automating the bulk of steps involved in creating a statistical report. The production of a RAP is not a trivial task given technical, time and knowledge limitations within teams who desire to replace traditional and semi-manual methods of statistical report production. We address some of the common knowledge gaps and hard-to-Google problems that upcoming RAP-pers face. This bookdown manual uses the eesectors package to help elucidate the RAP development process. It attempts to almagamate the learnings of the early pioneers of RAP into a maintained tome of knowledge to ensure good practice is accessible for RAP newbies. 1.1 Objectives Questions to ask oneself before you RAP? Why RAP? How to RAP? "],
["ethics.html", "Chapter 2 Ethical Considerations 2.1 Extra detail", " Chapter 2 Ethical Considerations The RAP is considered Data Science in that it involves an overlap of Computer Science, Statistics and Expert Domain Knowledge. Data science ethics are important. We consult the ethical framework before proceeding. We use the recommended Six Key Principles to structure our thinking: Start with clear user need and public benefit Use data and tools which have the minimum intrusion necessary Create robust data science models Be alert to public perceptions Be as open and accountable as possible Keep data secure Fundamentally, the public benefit of doing the project needs to be balanced against the risks of doing so. 2.1 Extra detail Working through the quick checklist in the Ethical Framework we can identify those Principles that are more pertinent to a RAP project. We discuss some of these considerations in more detail here and the questions you should ask your team and stakeholders before proceeding with development. 2.1.1 Start with clear user need and public benefit How does the department and public benefit? Is this a vanity project? How many statistical reports does your team produce? What proportion of their time does this take up? How much copy and pasting (data movement) between softwares is involved? Have you demonstrated the benefit to your team? (Show the thing.) RAP has reduced the time taken to produce a report from two months to two weeks. - placeholder 2.1.2 Use data and tools which have the minimum intrusion necessary How intrusive and identifiable is the data you are working with? What is the minimum data necessary to achieve the user benefit? Do you really need to use sensitive data (de-identify or aggregate it)? Have you made synthetic data, that looks like real data, for the RAP software development? If identifying individuals, how widely are you searching personal data? 2.1.3 Create robust data science models What is the quality of the data? How representative is the data? (what are the biases?) Are there any automated decisions? What safeguards can be implemented to validate any automatic decision making? What is the risk that someone will suffer a negative unintended consequence as a result of the project? What are all the data checking and validation steps that occur (if you can explain it to a human you should be able to code it)? 2.1.4 Be alert to public perceptions If personal data for operational purposes, how compatible was it with the reason collected? Do the public agree with what you are doing? Automation often has negative conotations - why is RAP useful to everyone involved? 2.1.5 Be as open and accountable as possible How open can you be about the project? How much oversight and accountability is there throughout the project? Can you explain in plain English why you are developing a RAP? Can you get colleagues from other departments to review your Pull Requests? 2.1.6 Keep data secure How secure is your data? You will be using a version control software like Git. How will you prevent your RAP developers from accidentally pushing sensitive data to Github? Are you following GDS best practice? Have you considered using tools, like Git hooks, to prevent the unintended publication of sensitive data on Github? "],
["why.html", "Chapter 3 Why RAP? 3.1 The current statistics production process", " Chapter 3 Why RAP? Reproducible Analytical Pipelines require a range of tools and techniques to implement that can be a challenge to overcome; so why bother learning to RAP? Does your team spend too much time moving data between various softwares? Could you reproduce your most recent publication’s stats? How about from five years ago? What would your team do with their time if it was freed up from copying and pasting? What proportion of spreadsheets contain errors? 3.1 The current statistics production process Here we use the production of official statistics in Government; altough this process varies widely perhaps it looks something like this? Broadly speaking, data are extracted from a datastore (whether it is a data lake, database, spreadsheet, or flat file), and are manipulated in a proprietary statistical software package, and possibly in proprietary spreadsheet software. Formatted tables are often then ‘copy and pasted’ into a word processor, before being converted to pdf format, and finally published to GOV.UK. This is quite a simplification, as statistical publications are usually produced by several people, so this process is likely to be happening in parallel many times. 3.1.1 The problem with this approach Errors in spreadsheets are common due to human error3. We can mitigate these issues by minimising the role humans play in the tasks that they perform poorly4. This will free up human time to focus on complex tasks such as the interpretation of the statistics and communicating the implications of these findings to others. Other issues are: Copy and pasting. Lack of proper version control. Testing and QA normally happens at the end. A key element in this process is quality assurance (QA). Each publication is meticulously checked to ensure the accuracy of the statistics being produced. This may take place throughout the production process or at the end prior to publication. Traditionally, QA has been a manual process which can take up a significant portion of the overall production time of a publication, as any changes will require the manual process of production to be repeated. 3.1.2 Desired Reproducible Analytical Pipeline Our analytical pipeline should have all the hallmarks of good scientific practice - it should be reproducible. Research on spreadsheet errors is substantial, compelling, and unanimous. It has three simple conclusions. The first is that spreadsheet errors are rare on a per-cell basis, but in large programs, at least one incorrect bottom-line value is very likely to be present. The second is that errors are extremely difficult to detect and correct. The third is that spreadsheet developers and corporations are highly overconfident in the accuracy of their spreadsheets.↩ Tasks that machines excel at↩ "],
["exemplar.html", "Chapter 4 Exemplar RAP 4.1 Package Purpose 4.2 eesectors Package Exploration 4.3 Chapter plenary", " Chapter 4 Exemplar RAP Chapter 3 considered why RAP is a useful paradigm. In this Chapter we demonstrate a RAP package developed in collaboration with the Department for Culture Media and Sport (DCMS). 4.1 Package Purpose In this exemplar project Matt Upson aimed at a high level of automation to demonstrate what is possible, and because DCMS had a skilled data scientist on hand to maintain and develop the project. Nonetheless, in the course of the work, statisticians at DCMS continue to undertake training in R, and the Better Use of Data Team spent time to ensure that the software development practices such as managing software dependencies, version control, package development, unit testing, style guide, open by default and continuous integration are embedded within the team that owns the publication. We’re continuing to support DCMS in the development of this prototype pipeline, with the expectation that it will be used operationally in 2017. If you want to learn more about this project, the source code for the eesectors R package is maintained on GitHub.com. The README provides instructions on how to test the package using the openly published data from the 2016 publication. 4.2 eesectors Package Exploration The following is an exploration of the eesectors package to help familiarise users with the key principles so that they can automate report production through package development in R using knitr. This examines the package in more detail compared to the README so that data scientists looking to implement RAP can note some of the characteristics of the code employed. 4.2.1 Installation The package can then be installed using devtools::install_github('ukgovdatascience/eesectors'). Some users may not be able to use the devtools::install_github() commands as a result of network security settings. If this is the case, eesectors can be installed by downloading the zip of the repository and installing the package locally using devtools::install_local(&lt;path to zip file&gt;). 4.2.1.1 Version control As the code is stored on Github we can access the current master version as well as all historic versions. This allows me to reproduce a report from last year if required. I can look at what release version was used and install that accordingly using the additional arguments for install_github. 4.2.2 Loading the package Installation means the package is on our computer but it is not loaded into the computer’s working memory. We also load any additional packages that might be useful for exploring the package or data therein. ## Skipping install of &#39;eesectors&#39; from a github remote, the SHA1 (5e91f359) has not changed since last install. ## Use `force = TRUE` to force installation ## eesectors: Reproducible Analytical Pipeline (RAP) for the ## Economic Estimates for DCMS Sectors Statistical First Release ## (SFR). For more information visit: ## https://github.com/ukgovdatascience/eesectors This makes all the functions within the package available for use. It also provides us with some R data objects, such as aggregated data sets ready for visualisations or analysis within the report. Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. - Hadely Wickham 4.2.3 Explore the package A good place to start is the package README. 4.2.3.1 Status badges The status badges provide useful information. They are found in the top left of the README and should be green and say passing. This indicates that this package will run OK on Windows and linux or mac. Essentially the package is likely to build correctly on your machine when you install it. You can carry out these build tests locally using the devtools package. 4.2.3.2 Look at the output first If you go to Chapter 3 of the DCMS publication it is apparent that most of the content is either data tables of summary statistics or visualisation of the data. This makes automation particularly useful here and likely to make time savings. Chapter 3 seems to be fairly typical in its length (if not a bit shroter compared to other Chapters). This package seems to work by taking the necessary data inputs as arguments in a function then outputting the relevant figures. The names of the functions match the figures they produce. Prior to this step we have to get the data in the correct format. If you look at the functions within the package within R Studio using the package navigator it is evident that there are a function of families dedicated to reading Excel spreadsheets and collecting the data in a tidy .Rds format. These are given the funciton name-prefix of extract_ (try to give your functions good names). The GVA_by_sector_2016 provides test data to work with during development. This will be important for the development of other packages for different reports. You need a precise understanding of how you go from raw data, to aggregated data (such as GVA_by_sector_2016) to the final figure. What are your inputs (arguments) and outputs? In some cases where your master data is stored in a particularly difficult for a machine to read you may prefer having a human to this extraction step. dplyr::glimpse(GVA_by_sector_2016) ## Observations: 54 ## Variables: 3 ## $ sector &lt;fctr&gt; creative, culture, digital, gambling, sport, telecoms,... ## $ year &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2... ## $ GVA &lt;dbl&gt; 65188, 20291, 97303, 8407, 7016, 24738, 49150, 69398, 2... x &lt;- GVA_by_sector_2016 4.2.3.3 Automating QA Human’s are not particularly good at Quality Assurance (QA), especially when working with massive spreadsheets it’s easy for errors to creep in. We can automate alot of the sense checking and update this if things change or a human provides another creative test to use for sense checking. If you can describe the test to a colleague then you can code it. The author uses messages to tell us what checks are being conducted or we can look at the body of the function if we are interested. This is useful if you are considering developing your own package, it will help you struture the message which are useful for the user. gva &lt;- year_sector_data(GVA_by_sector_2016) ## Initiating year_sector_data class. ## ## ## Expects a data.frame with three columns: sector, year, and measure, where ## measure is one of GVA, exports, or enterprises. The data.frame should include ## historical data, which is used for checks on the quality of this year&#39;s data, ## and for producing tables and plots. More information on the format expected by ## this class is given by ?year_sector_data(). ## ## *** Running integrity checks on input dataframe (x): ## ## Checking input is properly formatted... ## Checking x is a data.frame... ## Checking x has correct columns... ## Checking x contains a year column... ## Checking x contains a sector column... ## Checking x does not contain missing values... ## Checking for the correct number of rows... ## ...passed ## ## ***Running statistical checks on input dataframe (x)... ## ## These tests are implemented using the package assertr see: ## https://cran.r-project.org/web/packages/assertr for more details. ## Checking years in a sensible range (2000:2020)... ## Checking sectors are correct... ## Checking for outliers (x_i &gt; median(x) + 3 * mad(x)) in each sector timeseries... ## Checking sector timeseries: all_dcms ## Checking sector timeseries: creative ## Checking sector timeseries: culture ## Checking sector timeseries: digital ## Checking sector timeseries: gambling ## Checking sector timeseries: sport ## Checking sector timeseries: telecoms ## Checking sector timeseries: tourism ## Checking sector timeseries: UK ## ...passed ## Checking for outliers on a row by row basis using mahalanobis distance... ## Checking sector timeseries: all_dcms ## Checking sector timeseries: creative ## Checking sector timeseries: culture ## Checking sector timeseries: digital ## Checking sector timeseries: gambling ## Checking sector timeseries: sport ## Checking sector timeseries: telecoms ## Checking sector timeseries: tourism ## Checking sector timeseries: UK ## ...passed This is a semi-automated process so the user should check the Checks and ensure they meet their usual checks that would be conducted manually. If a new check or test becomes necessary then it should be implemented by changing the code. body(year_sector_data) ## { ## message(&quot;Initiating year_sector_data class.\\n\\n\\nExpects a data.frame with three columns: sector, year, and measure, where\\nmeasure is one of GVA, exports, or enterprises. The data.frame should include\\nhistorical data, which is used for checks on the quality of this year&#39;s data,\\nand for producing tables and plots. More information on the format expected by\\nthis class is given by ?year_sector_data().&quot;) ## message(&quot;\\n*** Running integrity checks on input dataframe (x):&quot;) ## message(&quot;\\nChecking input is properly formatted...&quot;) ## message(&quot;Checking x is a data.frame...&quot;) ## if (!is.data.frame(x)) ## stop(&quot;x must be a data.frame&quot;) ## message(&quot;Checking x has correct columns...&quot;) ## if (length(colnames(x)) != 3) ## stop(&quot;x must have three columns: sector, year, and one of GVA, export, or x&quot;) ## message(&quot;Checking x contains a year column...&quot;) ## if (!&quot;year&quot; %in% colnames(x)) ## stop(&quot;x must contain year column&quot;) ## message(&quot;Checking x contains a sector column...&quot;) ## if (!&quot;sector&quot; %in% colnames(x)) ## stop(&quot;x must contain sector column&quot;) ## message(&quot;Checking x does not contain missing values...&quot;) ## if (anyNA(x)) ## stop(&quot;x cannot contain any missing values&quot;) ## message(&quot;Checking for the correct number of rows...&quot;) ## if (nrow(x) != length(unique(x$sector)) * length(unique(x$year))) { ## warning(&quot;x does not appear to be well formed. nrow(x) should equal\\nlength(unique(x$sector)) * length(unique(x$year)). Check the of x.&quot;) ## } ## message(&quot;...passed&quot;) ## message(&quot;\\n***Running statistical checks on input dataframe (x)...\\n\\n These tests are implemented using the package assertr see:\\n https://cran.r-project.org/web/packages/assertr for more details.&quot;) ## value &lt;- colnames(x)[(!colnames(x) %in% c(&quot;sector&quot;, &quot;year&quot;))] ## message(&quot;Checking years in a sensible range (2000:2020)...&quot;) ## assertr::assert_(x, assertr::in_set(2000:2020), ~year) ## message(&quot;Checking sectors are correct...&quot;) ## sectors_set &lt;- c(creative = &quot;Creative Industries&quot;, culture = &quot;Cultural Sector&quot;, ## digital = &quot;Digital Sector&quot;, gambling = &quot;Gambling&quot;, sport = &quot;Sport&quot;, ## telecoms = &quot;Telecoms&quot;, tourism = &quot;Tourism&quot;, all_dcms = &quot;All DCMS sectors&quot;, ## perc_of_UK = &quot;% of UK GVA&quot;, UK = &quot;UK&quot;) ## assertr::assert_(x, assertr::in_set(names(sectors_set)), ## ~sector, error_fun = raise_issue) ## message(&quot;Checking for outliers (x_i &gt; median(x) + 3 * mad(x)) in each sector timeseries...&quot;) ## series_split &lt;- split(x, x$sector) ## lapply(X = series_split, FUN = function(x) { ## message(&quot;Checking sector timeseries: &quot;, unique(x[[&quot;sector&quot;]])) ## assertr::insist_(x, assertr::within_n_mads(3), lazyeval::interp(~value, ## value = as.name(value)), error_fun = raise_issue) ## }) ## message(&quot;...passed&quot;) ## message(&quot;Checking for outliers on a row by row basis using mahalanobis distance...&quot;) ## lapply(X = series_split, FUN = maha_check) ## message(&quot;...passed&quot;) ## structure(list(df = x, colnames = colnames(x), type = colnames(x)[!colnames(x) %in% ## c(&quot;year&quot;, &quot;sector&quot;)], sector_levels = levels(x$sector), ## sectors_set = sectors_set, years = unique(x$year)), class = &quot;year_sector_data&quot;) ## } The function is structured to tell the user what check is being made and then running that check given the input x. If the input fails a check the function is stopped with a useful diagnostic message for the user. This is achieved using if and the opposite of the desired feature of x. message(&quot;Checking x has correct columns...&quot;) if (length(colnames(x)) != 3) stop(&quot;x must have three columns: sector, year, and one of GVA, export, or x&quot;) For example, if x does not have exactly three columns we stop. 4.2.3.4 Output of this function The output object is different to the input as expected, yet it does contain the initial data. identical(gva$df, x) ## [1] TRUE The rest of the list contains other details that could be changed at a later date if required, demonstrating defensive programming. For example, the sectors that are of interest to DCMS have changed and may change again. ?year_sector_data Let’s take a closer look at this function using the help and other standard function exploration functions. The help says it produces a custom class of object with five slots. isS4(gva) ## [1] FALSE class(gva) ## [1] &quot;year_sector_data&quot; It’s not actually an S4 object, by slots the author means a list of objects. This approach is sensible and easy to work with, as most users are familiar with S3. 4.2.3.5 The input The input, which is likely a bunch of not tidy or messy spreadsheets needs to be wrangled and aggregated (if necessary) for input into the functions prefixed by figure. dplyr::glimpse(GVA_by_sector_2016) ## Observations: 54 ## Variables: 3 ## $ sector &lt;fctr&gt; creative, culture, digital, gambling, sport, telecoms,... ## $ year &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2... ## $ GVA &lt;dbl&gt; 65188, 20291, 97303, 8407, 7016, 24738, 49150, 69398, 2... 4.2.3.6 The R output With the data in the appropriate form to be received as an argument or input for the figure family of functions we can proceed to plot. figure3.1(x = gva) Again we can look at the details of the plot. We could change the body of the function to affect change to the default plot or we can pass additional ggplot arguments to it. Reading the code we see it filters the data, makes the variables it needs, refactors the sector variable and then plots it. body(figure3.1) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~sector != &quot;UK&quot;) ## x &lt;- dplyr::mutate_(x, year = ~factor(year, levels = c(2016:2010))) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)]), ## levels = rev(as.character(unname(sectors_set[levels(x$sector)])))) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~GVA, x = ~sector, ## fill = ~year) + ggplot2::geom_bar(colour = &quot;slategray&quot;, ## position = &quot;dodge&quot;, stat = &quot;identity&quot;) + ggplot2::coord_flip() + ## govstyle::theme_gov(base_colour = &quot;black&quot;) + ggplot2::scale_fill_brewer(palette = &quot;Blues&quot;) + ## ggplot2::ylab(&quot;Gross Value Added (£bn)&quot;) + ggplot2::theme(legend.position = &quot;right&quot;, ## legend.key = ggplot2::element_blank()) + ggplot2::scale_y_continuous(labels = scales::comma) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.1():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.1():&quot;, e) ## }, finally = { ## }) ## } We can inspect and change an argument if we feel inclined or if a new colour scheme becomes preferred for example. However, there is no ... in the body of the function itself so where does this argument get passed to? This all looks straight forward and we can inspect the other functions for generating the figures or plot output. body(figure3.2) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~sector %in% c(&quot;UK&quot;, &quot;all_dcms&quot;)) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)])) ## x &lt;- dplyr::group_by_(x, ~sector) ## x &lt;- dplyr::mutate_(x, index = ~max(ifelse(year == 2010, ## GVA, 0)), indexGVA = ~GVA/index * 100) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~indexGVA, ## x = ~year, colour = ~sector, linetype = ~sector) + ## ggplot2::geom_path(size = 1.5) + govstyle::theme_gov(base_colour = &quot;black&quot;) + ## ggplot2::scale_colour_manual(values = unname(govstyle::gov_cols[c(&quot;red&quot;, ## &quot;purple&quot;)])) + ggplot2::ylab(&quot;GVA Index: 2010=100&quot;) + ## ggplot2::theme(legend.position = &quot;bottom&quot;, legend.key = ggplot2::element_blank()) + ## ggplot2::ylim(c(80, 130)) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.2():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.2():&quot;, e) ## }, finally = { ## }) ## } body(figure3.3) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~!sector %in% c(&quot;UK&quot;, &quot;all_dcms&quot;)) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)])) ## x &lt;- dplyr::group_by_(x, ~sector) ## x &lt;- dplyr::mutate_(x, index = ~max(ifelse(year == 2010, ## GVA, 0)), indexGVA = ~GVA/index * 100) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~indexGVA, ## x = ~year, colour = ~sector, linetype = ~sector) + ## ggplot2::geom_path(size = 1.5) + govstyle::theme_gov(base_colour = &quot;black&quot;) + ## ggplot2::scale_colour_brewer(palette = &quot;Set1&quot;) + ## ggplot2::ylab(&quot;GVA Index: 2010=100&quot;) + ggplot2::theme(legend.position = &quot;right&quot;, ## legend.key = ggplot2::element_blank()) + ggplot2::ylim(c(80, ## 150)) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.2():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.2():&quot;, e) ## }, finally = { ## }) ## } 4.2.3.7 Error handling A point of interest in the code with which some users may be unfamiliar is tryCatch which is a function that allows the function to catch conditions such as warnings, errors and messages. We see this towards the end of the function where if either of these conditions are produced then an informative message is produced (in that it tells you in what function there was a problem). The structure here is simple and could be copied and pasted for use in automating other figures of other chapters or statistical reports. For a comprehensive introduction see Hadley’s Chapter. 4.3 Chapter plenary We have explored the eesectors package from the perspective of someone wishing to develop our own semi-automated chapter production through the development of a package in R. This package provides a useful tempplate where one could copy the foundations of the package and workflow. "],
["open.html", "Chapter 5 Open Source rather than proprietary", " Chapter 5 Open Source rather than proprietary Open source languages such as Python and R are increasing in popularity across government. One advantage of using these tools is that we can reduce the number of steps where the data needs to be moved from one program (or format) into another. This is in line with the principle of reproducibility given in guidance on producing quality analysis for government (the AQUA book), as the entire process can be represented as a single step in code, greatly reducing the likelihood of manual transcription errors. Chapter 3 discussed why spreadsheets are dangerous. Moving away from proprietary software, towards Open Source, may also have the additional benefit of being more compatable with tried and tested software development tools and techniques (as well as the obvious no longer needing to pay for it). In GDS’s project with DCMS described in Chapter 4, we decided to use the R language, however we could equally have chosen to use Python or another language entirely: the techniques we outline in this book are language agnostic. "],
["vs.html", "Chapter 6 Version Control 6.1 Introduction 6.2 Useful resources 6.3 Typical workflow", " Chapter 6 Version Control 6.1 Introduction Few software engineers would embark on a new project without using some sort of version control software. Version control software allows us to track the three Ws: Who made Which change, and Why?. Tools like git can be used to track files of any type, but are particularly useful for code in text files for example R or Python code. Whilst git can be used locally on a single machine, or many networked machines, git can also be hooked up to free cloud services such as GitHub, GitLab, or Bitbucket(https://bitbucket.org/). Each of these services provides hosting for your version control repository, and makes the code open and easy to share. The entire project we are working on with DCMS can be seen on GitHub. Obviously this won’t be appropriate for all Government projects (and solutions do exist to allow these services to be run within secure systems), but in our work with DCMS, we were able to publish all of our code openly. You can use our code to run an example based on the 2016 publication, but producing the entire publication from end to end would require access to data which is not published openly. Below is a screenshot from the commit history showing collaboration between data scientists in DCMS and GDS. The full page can be seen on GitHub. Using a service like GitHub allows us to formalise the system of quality assurance (QA) in an auditable way. We can configure GitHub to require a code review by another person before the update to the code (this is called a pull request) is accepted into the main workstream of the project. You can see this in the screenshot below which relates to a pull request which fixed a minor bug in the prototype. The work to fix it was done by a data scientist at DCMS, and reviewed by a data scientist from GDS. The open nature of the good is great for transparency and facilitates review. The entire community can contribute to helping QA your code and identify issues or bugs. If you are lucky, they will not only report the bug / issue, but may also offer a fix for your code in the form of a pull request. 6.2 Useful resources 6.2.1 Graphical user interface focus A useful book on Git and Github that should cover all your needs for those who are uncomfortable working in a command line interface. This will cover most of your Git and Github workflow needs for collaborating in a team. 6.2.2 Command line focus However, the terminal isn’t that scary really and we recommend using it from the outset. Here’s a video tutorial that provides a good introduction and does not expect any experience of using the Unix shell (the terminal or command line). For a comprehensive tome try the Pro Git book. 6.3 Typical workflow When you first start using git it can be difficult to remember all the commonly used commands. We give a simple workflow here (assuming you are collaborating on Github and have set up a repo called my_repo with the origin and remote set (try to avoid hyphens in names)). Remember to remove the comments when copying and pasting into the terminal. You will also need to give your new feature branch a good name. Open your terminal. Navigate to my_repo using cd. Check you are up-to-date: # git checkout master # git pull Create your new feature branch to work on: # git checkout -b feature/post_name Squash your commits if appropriate, then push your new branch to Github. # git push origin feature/post_name -u On Github create a pull request and ask a colleague to review your changes before merging with the master branch (you can assign a reviewer in the PR page on Github). If accepted you’re new feature will have been merged on Github. Fetch these changes locally. # git checkout master # git pull "],
["package.html", "Chapter 7 Packaging Code 7.1 Essential reading", " Chapter 7 Packaging Code A package enshrines all the business knowledge used to create a corpus of work in one place; including the code and its relevant documentation. One of the difficulties that can arise in the more manual methods of statistics production is that we have many different files relating to many different stages of the process, each of which needs to be documented, and kept up to date. Part of the heavy lifting can be done here with version control as described in Chapter 6, but we can go a step further: we can create a package of code. As Hadley Wickham (author of a number of essential packages for package development) puts it for R: Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. - Hadley Wickham Since it is a matter of statute that we produce our statistical publications, it is essential that our publications are as reproducible as possible. Packaging up the code can also help with institutional knowledge transfer. This was exemplified in Chapter 4 where we explored help files associated with code using the R ? function. library(eesectors) ?clean_sic() Linking the documentation to the code makes everything much easier to understand, and can help to minimising the time taken to bring new team members up to speed. This all meets the requirements of the AQUA book in that all assumptions and constraints can be described in the package documentation asssociated tied to the relevant code. 7.1 Essential reading Hadley Wickham’s R Packages book is an excellent and comprehensive introduction to developing your own package in R. It encourages you to start with the basics and improve over time; good advice. "],
["test.html", "Chapter 8 Unit test 8.1 Further Reading", " Chapter 8 Unit test Testing is a vital part of package development. It ensures that your code does what you want it to do. We can facilitate testing and Quality Assurance (QA) by building packages with generalisable LEGO-like functions. In procedural programming, code is designed to be reused again and again with different inputs, making for simpler code that is easier to understand and audit. It means we can easily build tests to ensure the code continues to work as expected when we make changes to it. Since each function or group of functions (unit) is generic, it can be tested with a generic example, so that we know that our unit of code works as expected. If we discover cases where our units do not do perform as expected, we can codify these cases into new tests and work to fix the problem until the test passes. We may even go a step further and adopt the practice of test driven development: starting each unit of code with a test which fails, until we write code which can pass the test. Test-driven development is a useful paradigm for developing your code in a thoroughly QA’d fashion. 8.1 Further Reading Much of the heavy lifting for this kind of testing can be done in a unit testing framework, for example testthat for R, or nosetools in Python. For how to test in R read the R package, Testing Chapter "],
["travis.html", "Chapter 9 Automated Testing", " Chapter 9 Automated Testing Once we are in the habit of packaging our code and writing unit tests, we can start to use free online tools such as Travis CI, Jenkins, or Appveyor to automatically test that our package of code builds, and that the tests we have written, pass. These tools integrate with GitHub, so we can easily see when an update to the code has failed one of our tests. The green ticks in the box below show that our tests have passed for the given pull request. We can also look at our test history on Travis CI, as in the screenshot below. From this we can see that our main workstream - the default branch - has been tested 619 times to date, the last of which was one day ago, and the previous five tests have all passed without problems. "],
["code-cover.html", "Chapter 10 Code coverage", " Chapter 10 Code coverage An additional set of tools we can start to use once we begin writing our own tests is code coverage tools, for instance codecov.io, or coveralls.io. These tools are able to analyse the code we have written via hosting services like GitHub, and provide a line by line breakdown of which lines are tested, and which are not. For example, in the lines below from the file year_sector_table.R, we can see that lines 112-115 and 117-120 are not explicitly tested. In this case, we probably don’t need to worry very much, but on other occasions this might prompt us to write more tests. "],
["dependency-management-dep.html", "Chapter 11 Dependency management {dep}", " Chapter 11 Dependency management {dep} If you want your code to be reproducible in the long-run (i.e. so you can come back to run it next month or next year), you’ll need to track the versions of the packages that your code uses. A rigorous approach is to use packrat, http://rstudio.github.io/packrat/, which store packages in your project directory, or checkpoint, https://github.com/RevolutionAnalytics/checkpoint, which will reinstall packages available on a specified date. A quick and dirty hack is to include a chunk that runs sessionInfo() — that won’t you let easily recreate your packages as they are today, but at least you’ll know what they were. - Hadley Wickham, R for Data Science One of the problems with working with open source software is that it is quite easy to fall into a trap called ‘dependency hell’. Essentially, this occurs when the software we write depends on open source packages, which depend on other open source packages, which can depend on other packages, and on, and on. All these packages may be written by many different people, and are updated at vastly different timescales. If we fail to take account of this, then we are likely to fail at the first hurdle of reproducibility, and we may find that in a year’s time we are no longer able to reproduce the work that we previously did - or at least not without a lot of trouble. There are several ways we might get on top of this problem, but in this project we opted for using packrat, which creates a cache of all the R packages used in the project which is then version controlled on GitHub. Matt Upson has blogged about this previously in the context of writing academic works. "],
["qa-data.html", "Chapter 12 Quality Assured data", " Chapter 12 Quality Assured data All the testing we have described so far is to do with the code, and ensuring that the code does what we expect it to, but because we have written an R package, it’s also very easy for us to institute tests for the consistency of the data at the time the data is loaded. The list of tests that we might want to run is endless, and the scope of tests very much be dictated by the team which has the expert knowledge of the data. In the eesectors package we implemented two very simple checks, but these could very easily be expanded. The simplest of these is a simple test for outliers: since the data for the economic estimates is longitudinal, i.e. stretching back several years; we are able to look at the most recent values in comparison to the values from previous years. If the latest values lie within a threshold determined statistically from the other values then the data passes, if not a warning is raised. These kinds of automated tests are repeated every time the data are loaded, reducing the burden of QA, and the scope for human error, freeing up statistician time for identifying more subtle data quality issues which might otherwise go unnoticed. "],
["pub.html", "Chapter 13 Producing the publication 13.1 Further Reading", " Chapter 13 Producing the publication R Markdown provides an unified authoring framework for data science, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. - Hadley Wikcham, R for Data Science Everything I have talked about so far is to do with the production of the statistics themselves, not preparation of the final publication, but there are tools that can help with this too. In our project with DCMS we plan to use Rmarkdown (a flavour of markdown) to incorporate the R code into the same document as the text of the publication. Working in this way means that we can do all of the operations in a single file, so we have no problems with ensuring that our tables or figures are synced with the latest version of the text: everything can be produced in a single file. We can even produce templates with boilerplate text like: ‘this measure increased by X%’, and then automatically populate the X with the correct values when we run the code. 13.1 Further Reading You can write individual chapters using R markdown, as one file per Chapter. Alternatively you can write the whole publication using bookdown. For a basic start in bookdown try this blog post (we wrote this book using this to kick things off). "],
["final.html", "Chapter 14 Final Thoughts", " Chapter 14 Final Thoughts It’s called Data Science for a reason, treat all the data handling, experimentation and analysis in your lab-notebook. "]
]
