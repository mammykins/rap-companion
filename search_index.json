[
["index.html", "RAP Companion Preface 0.1 Discovery 0.2 Getting the balance right 0.3 Inspiration", " RAP Companion Matthew Gregory, Matthew Upson 2019-01-09 Preface Producing official statistics for publications is a key function of many teams across Government. It’s a time consuming and meticulous process to ensure that statistics are accurate and timely. With open source software becoming more widely used, there’s now a range of tools and techniques that can be used to reduce production time, whilst maintaining and even improving the quality of the publications. This book is about these techniques: what they are, and how we can use them. 0.1 Discovery Something is better than nothing. This is NOT TRUE in the agile world; consider the opportunity cost1. Prior to commiting to RAP development in your team ask yourself two questions: Is there a genuine user need for this new product/service? Does another product/service exist that you can re-use or adapt?2 0.2 Getting the balance right Whilst incredibly powerful, these approaches should not be seen as panacea for all the difficulties of statistics production: however, implementing even a few of these techniques can drive benefits in auditability, speed, quality, and knowledge transfer. There is a balance to be struck between ease of maintenance and the level of automation: this is likely to differ for every publication or team. These techniques are however tried and tested for software development and most already feature in the Service Manual: in this project we have just applied these methodologies to a new area. 0.3 Inspiration This book was inspired by, and draws heavily from a Government Digital Service blog post by Dr. Matthew Upson, itself taking much inspiration from the fields of DevOps and reproducible research. the loss of other alternatives when one alternative is chosen↩ There’s probably some code you can adapt on Github, such as the eesectors pacakge↩ "],
["intro.html", "Chapter 1 Introduction 1.1 Objectives", " Chapter 1 Introduction The Reproducible Analytical Pipeline (RAP) is an alternative production methodology for automating the bulk of steps involved in creating a statistical report. The production of a RAP is not a trivial task given technical, time and knowledge limitations within teams who desire to replace traditional and semi-manual methods of statistical report production. We address some of the common knowledge gaps and hard-to-Google problems that upcoming RAP-pers face. This bookdown manual uses the eesectors package to help elucidate the RAP development process. It attempts to amalgamate the learnings of the early pioneers of RAP into a maintained tome of knowledge to ensure good practice is accessible for RAP newbies. To complement this book, one of our RAPpers has developed a Massive Online Open Course to share an approach to learning this technical skill-set. This course is an informal introduction and describes the best practices through the use of screencasts and assignments. It is currently available on Udemy and takes you through the RAP journey using a simple RAP example. 1.1 Objectives Questions to ask oneself before you RAP? Why RAP? How to RAP? "],
["ethics.html", "Chapter 2 Ethical Considerations 2.1 Extra detail", " Chapter 2 Ethical Considerations The RAP is considered Data Science in that it involves an overlap of Computer Science, Statistics and Expert Domain Knowledge. Data science ethics are important. We consult the ethical framework before proceeding. We use the recommended Six Key Principles to structure our thinking: Start with clear user need and public benefit Use data and tools which have the minimum intrusion necessary Create robust data science models Be alert to public perceptions Be as open and accountable as possible Keep data secure Fundamentally, the public benefit of doing the project needs to be balanced against the risks of doing so. 2.1 Extra detail Working through the quick checklist in the Ethical Framework we can identify those Principles that are more pertinent to a RAP project. We discuss some of these considerations in more detail here and the questions you should ask your team and stakeholders before proceeding with development. 2.1.1 Start with clear user need and public benefit How does the department and public benefit? Is this a vanity project? How many statistical reports does your team produce? What proportion of their time does this take up? How much copy and pasting (data movement) between softwares is involved? Have you demonstrated the benefit to your team? (Show the thing.) RAP has reduced the time taken to produce a report from two months to two weeks. - placeholder 2.1.2 Use data and tools which have the minimum intrusion necessary How intrusive and identifiable is the data you are working with? What is the minimum data necessary to achieve the user benefit? Do you really need to use sensitive data (de-identify or aggregate it)? Have you made synthetic data, that looks like real data, for the RAP software development? If identifying individuals, how widely are you searching personal data? 2.1.3 Create robust data science models What is the quality of the data? How representative is the data? (what are the biases?) Are there any automated decisions? What safeguards can be implemented to validate any automatic decision making? What is the risk that someone will suffer a negative unintended consequence as a result of the project? What are all the data checking and validation steps that occur (if you can explain it to a human you should be able to code it)? 2.1.4 Be alert to public perceptions If personal data for operational purposes, how compatible was it with the reason collected? Do the public agree with what you are doing? Automation often has negative conotations - why is RAP useful to everyone involved? 2.1.5 Be as open and accountable as possible How open can you be about the project? How much oversight and accountability is there throughout the project? Can you explain in plain English why you are developing a RAP? Can you get colleagues from other departments to review your Pull Requests? 2.1.6 Keep data secure How secure is your data? You will be using a version control software like Git. How will you prevent your RAP developers from accidentally pushing sensitive data to Github? Are you following GDS best practice? Have you considered using tools, like Git hooks, to prevent the unintended publication of sensitive data on Github? "],
["why.html", "Chapter 3 Why RAP? 3.1 The current statistics production process 3.2 Why learning to RAP might be hard", " Chapter 3 Why RAP? Reproducible Analytical Pipelines require a range of tools and techniques to implement that can be a challenge to overcome; so why bother learning to RAP? Does your team spend too much time moving data between various softwares? Could you reproduce your most recent publication’s stats? How about from five years ago? What would your team do with their time if it was freed up from copying and pasting? What proportion of spreadsheets contain errors? 3.1 The current statistics production process Here we use the production of official statistics in Government; altough this process varies widely perhaps it looks something like this? Broadly speaking, data are extracted from a datastore (whether it is a data lake, database, spreadsheet, or flat file), and are manipulated in proprietary statistical software, and possibly in proprietary spreadsheet software. Formatted tables are often then ‘copy and pasted’ into a word processor, before being converted to pdf format, and finally published to GOV.UK. This is quite a simplification, as statistical publications are usually produced by several people, so this process is likely to be happening in parallel many times. Often quality assurance (QA) is a process takes place at the end of the process. At its simplest, this may be a word processor document or spreadhseet sent by email to colleagues for review, prior to being approved for publication. 3.1.1 The problem with this approach Errors in spreadsheets are common due to human error3. There are plenty of spreadsheet horror stories to learn from and encourage us to try a different approach. We can mitigate these issues by minimising the role humans play in the tasks that they perform poorly4. This will free up human time to focus on complex tasks such as the interpretation of the statistics and communicating the implications of these findings to others. Other issues are: Copy and pasting. Lack of proper version control. Testing and QA often happens at the end, not throughout. A key element in this process is quality assurance. Each publication is meticulously checked to ensure the accuracy of the statistics being produced. This may take place throughout the production process or at the end prior to publication. Traditionally, QA has been a manual process which can take up a significant portion of the overall production time of a publication, as any changes will require the manual process of production to be repeated. 3.1.2 Desired Reproducible Analytical Pipeline An analytical pipeline should have all the hallmarks of good scientific practice - it should be reproducible. We should make use of modern DataOps principles, tools and techniques to improve the quality of the final product in a more timely fashion compared to the traditional method. All these parallel work flows to produce different parts of the report (e.g. tables and figures represented by different colours) can be conducted by bespoke functions within an R software package (we are language agnostic but prefered R and Rmarkdown for our DCMS work due to their prior experience with R). Rather than copying and pasting between different software, we read our data in once using a tidy data set that we create. We then do all the analysis and report production within R and benefit from many open source tools that make up RAP. These are represented by the logos along the bottom; git, Github, Travis, Appveyor and codecov. 3.2 Why learning to RAP might be hard Automating statistical report production is not without its challenges. RAP requires changes to your processes and is dependent on tools and techniques without which your journey may fail. Based on ours and others’ experiences, here are some things that will make your road to RAP difficult (though not impossible): Data format changes unexpectedly: e.g. the source data are not stored in a relational database, but are provided from a third party in spreadsheets. Publication format changes frequently. Limited access to open source tools; ideally you would need access to the following: Rstudio R Rtools (required to build packages) git (command line or GUI - this is extremely important) github (need to be able to push and pull to/from a remote repository) Not coding in the open. Not coding in the open makes it much harder to collaborate, as you will not have free access to a range of automated tools like travis/appveyor/codecov/coveralls which are part of the RAP model. Business as usual (BAU): It’s not possible to do this kind of work whilst also doing BAU tasks. It takes time to get to grips with the techniques, and the first version should be considered a prototype that needs to be at least partly validated against a manual procedure. Research on spreadsheet errors is substantial, compelling, and unanimous. It has three simple conclusions. The first is that spreadsheet errors are rare on a per-cell basis, but in large programs, at least one incorrect bottom-line value is very likely to be present. The second is that errors are extremely difficult to detect and correct. The third is that spreadsheet developers and corporations are highly overconfident in the accuracy of their spreadsheets.↩ Tasks that machines excel at↩ "],
["exemplar.html", "Chapter 4 Exemplar RAP 4.1 Package Purpose 4.2 Tidy data 4.3 eesectors Package Exploration 4.4 Chapter plenary", " Chapter 4 Exemplar RAP Chapter 3 considered why RAP is a useful paradigm. In this Chapter we demonstrate a RAP package developed in collaboration with the Department for Culture Media and Sport (DCMS). This package enshrines all the pertinent business knowledge in one corpus. 4.1 Package Purpose In this exemplar project Matt Upson aimed at a high level of automation to demonstrate what is possible, and because DCMS had a skilled data scientist on hand to maintain and develop the project. Nonetheless, in the course of the work, statisticians at DCMS continue to undertake training in R, and the Better Use of Data Team spent time to ensure that the software development practices such as managing software dependencies, version control, package development, unit testing, style guide, open by default and continuous integration are embedded within the team that owns the publication. We’re continuing to support DCMS in the development of this prototype pipeline, with the expectation that it will be used operationally in 2017. If you want to learn more about this project, the source code for the eesectors R package is maintained on GitHub.com. The README provides instructions on how to test the package using the openly published data from the 2016 publication. 4.2 Tidy data Tidy data are all alike; every messy data is messy in its own way. - Hadley Tolstoy What is the simplest representation of the data possible? Prior to any analysis we must tidy our data: structuring our data to facilitate analysis. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. You and your team trying to RAP should spend time reading this paper and hold a seminar discussing it. It’s important to involve the analysts involved in the traditional production of this report as they will be familiar with the inputs and outputs of the report. With the heuristic of a tidy dataset in your mind, proceed, as a team, to look through the chapter or report you are attempting to produce using RAP. As you work through, note down what variables you would need to produce each table or figure, what would the input dataframe look like? (Say what you see.) After looking at all the figures and tables, is there one tidy daaset that could be used as input? Sketch out what it looks like. 4.2.1 eesectors tidy data We demonstrate this process using the DCMS publication, refer to Chapter 3 - GVA. What data do you need to produce this table? Variables: Year, Sector, GVA What data do you need to produce this figure? The GVA of each Sector by Year. Variables: Year, Sector, GVA What data do you need to produce this figure? Total GVA across all sectors. Variables: Year, Sector, GVA What data do you need to produce this figure? For each Year by Sector we need the GVA. Variables: Year, Sector, GVA 4.2.2 What does our eesectors tidy data look like? Remember, for tidy data: 1. Each variable forms a column. 2. Each observation forms a row. 3. Each type of observational unit forms a table. Our tidy data is of the form Year - Sector - GVA: Year Sector GVA 2010 creative 65188 2010 culture 20291 2010 digital 97303 2011 creative 69398 2011 culture 20954 2011 digital 107303 This data is for demonstration purposes only. 4.2.2.1 Another worked example - what does our SEN tidy data look like? We repeat the process above for a different publication related to Special Educational Needs data to demonstrate the thought process. We suggest you attempt to do this independently without peeking at the solution below, that way you can test your understanding. Look at the final report; work through and think about what data you need to produce each figure or table (write out the variables then sketch the minimal tidy data set required to build it). Ideally there will be one minimal tidy data set that we can build as input for our functions to produce these figures, tables or statistics. If a report covers a broad topic it might not be possible to have one minimal tidy data set (it’s OK to have more than one). We can create our own custom class of object to cope and keep things simple for the user of our package. Here we draw our tables in a pseudo csv format to digitise for sharing. Sketching with pencil and paper is also fine and much clearer! I also use shorthand for some of the variable names given in the publication. 4.2.2.1.1 Figure A year, all students, total sen, sen without statement or EHC plan, sen with statement or EHC plan … 4.2.2.1.2 Figure B This digs deeper than Fig A by counting and categorising students (converted into percentage) by their primary type of need. Thus our minimal table above will not meet the needs for Figure B. We’ll add in some example made-up data to check I understand the data correctly (the type of the data is the important thing e.g. date, integer, string). It’s important here to have expert domain knowledge as one might misunderstand due to esoteric language use. year, sen_status, sen_category, sen_primary_type_need, count 2016, 0, NA, NA, 3e6 2016, 1, “SEN Support”, “Specific Learning Difficulty”, 5000 2016, 1, “Statement or EHC Plan”, “Specific Learning Difficulty”, 1500 2016, 1, “SEN Support”, “Moderate Learning Difficulty”, 5000 2017, … ** Question: using the data table above can you produce both Figure A and B? ** With our data structured like this we have all the data we need to produce Figure B and Figure A. 4.2.2.1.3 Figure C Again we dig deeper and ask what’s their school type? We don’t have this in our previous minimal data table so we need to include this variable in our dataframe. year, school_type, sen_status, sen_category, sen_primary_type_need, count 2016, “State-funded primary”, 0, NA, NA, 3e6 2016, “State-funded primary”, 1, “SEN Support”, “Specific Learning Difficulty”, 5000 2016, “State-funded primary”, 1, “Statement or EHC Plan”, “Specific Learning Difficulty”, 1500 2016, “State-funded primary”, 1, “SEN Support”, “Moderate Learning Difficulty”, 5000 … As you can imagine the table can end up being quite long! ** Question: using the data table above can you produce both Figure A, B and C? ** Yes. Continue this thought process for the rest of the document. However, bear in mind that you have the added insight of where the data comes from and in what format, this might affect your using more than one data class for the package. For example you could call the one we described above as your “year-sch-sen” class, and have another data class dedicated to being the input for some of the other figures in the chapter. The data might come from an SQL query or a bunch of disparate spreadsheets. In the later case we can write some functions to extract and combine the data into a minimal tidy data table for use in our package. See eesectors README for an example. 4.2.3 How to build your tidy data? With the minimal tidy dataset idea in place, you can begin to think about how you might construct this tidy dataset from the data stores you have availiable. As we are working in R we can formalise this minimal tidy dataset as a class. For our eesectors package we create our long data year_sector_data class as the fundamental input to create all our figures and tables for the output report. 4.3 eesectors Package Exploration The following is an exploration of the eesectors package to help familiarise users with the key principles so that they can automate report production through package development in R using knitr. This examines the package in more detail compared to the README so that data scientists looking to implement RAP can note some of the characteristics of the code employed. 4.3.1 Installation The package can then be installed using devtools::install_github('ukgovdatascience/eesectors'). Some users may not be able to use the devtools::install_github() commands as a result of network security settings. If this is the case, eesectors can be installed by downloading the zip of the repository and installing the package locally using devtools::install_local(&lt;path to zip file&gt;). 4.3.1.1 Version control As the code is stored on Github we can access the current master version as well as all historic versions. This allows me to reproduce a report from last year if required. I can look at what release version was used and install that accordingly using the additional arguments for install_github. 4.3.2 Loading the package Installation means the package is on our computer but it is not loaded into the computer’s working memory. We also load any additional packages that might be useful for exploring the package or data therein. devtools::install_github(&#39;ukgovdatascience/eesectors&#39;) ## eesectors: Reproducible Analytical Pipeline (RAP) for the ## Economic Estimates for DCMS Sectors Statistical First Release ## (SFR). For more information visit: ## https://github.com/ukgovdatascience/eesectors This makes all the functions within the package available for use. It also provides us with some R data objects, such as aggregated data sets ready for visualisations or analysis within the report. Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. - Hadely Wickham 4.3.3 Explore the package A good place to start is the package README. 4.3.3.1 Status badges The status badges provide useful information. They are found in the top left of the README and should be green and say passing. This indicates that this package will run OK on Windows and linux or mac. Essentially the package is likely to build correctly on your machine when you install it. You can carry out these build tests locally using the devtools package. 4.3.3.2 Look at the output first If you go to Chapter 3 of the DCMS publication it is apparent that most of the content is either data tables of summary statistics or visualisation of the data. This makes automation particularly useful here and likely to make time savings. Chapter 3 seems to be fairly typical in its length (if not a bit shroter compared to other Chapters). This package seems to work by taking the necessary data inputs as arguments in a function then outputting the relevant figures. The names of the functions match the figures they produce. Prior to this step we have to get the data in the correct format. If you look at the functions within the package within R Studio using the package navigator it is evident that there are a function of families dedicated to reading Excel spreadsheets and collecting the data in a tidy .Rds format. These are given the funciton name-prefix of extract_ (try to give your functions good names). The GVA_by_sector_2016 provides test data to work with during development. This will be important for the development of other packages for different reports. You need a precise understanding of how you go from raw data, to aggregated data (such as GVA_by_sector_2016) to the final figure. What are your inputs (arguments) and outputs? In some cases where your master data is stored in a particularly difficult for a machine to read you may prefer having a human to this extraction step. dplyr::glimpse(GVA_by_sector_2016) ## Observations: 54 ## Variables: 3 ## $ sector &lt;fctr&gt; creative, culture, digital, gambling, sport, telecoms,... ## $ year &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2... ## $ GVA &lt;dbl&gt; 65188, 20291, 97303, 8407, 7016, 24738, 49150, 69398, 2... x &lt;- GVA_by_sector_2016 4.3.3.3 Automating QA Human’s are not particularly good at Quality Assurance (QA), especially when working with massive spreadsheets it’s easy for errors to creep in. We can automate alot of the sense checking and update this if things change or a human provides another creative test to use for sense checking. If you can describe the test to a colleague then you can code it. The author uses messages to tell us what checks are being conducted or we can look at the body of the function if we are interested. This is useful if you are considering developing your own package, it will help you struture the message which are useful for the user. gva &lt;- year_sector_data(GVA_by_sector_2016) ## Initiating year_sector_data class. ## ## ## Expects a data.frame with three columns: sector, year, and measure, where ## measure is one of GVA, exports, or enterprises. The data.frame should include ## historical data, which is used for checks on the quality of this year&#39;s data, ## and for producing tables and plots. More information on the format expected by ## this class is given by ?year_sector_data(). ## ## *** Running integrity checks on input dataframe (x): ## ## Checking input is properly formatted... ## Checking x is a data.frame... ## Checking x has correct columns... ## Checking x contains a year column... ## Checking x contains a sector column... ## Checking x does not contain missing values... ## Checking for the correct number of rows... ## ...passed ## ## ***Running statistical checks on input dataframe (x)... ## ## These tests are implemented using the package assertr see: ## https://cran.r-project.org/web/packages/assertr for more details. ## Checking years in a sensible range (2000:2020)... ## Checking sectors are correct... ## Checking for outliers (x_i &gt; median(x) + 3 * mad(x)) in each sector timeseries... ## Checking sector timeseries: all_dcms ## Checking sector timeseries: creative ## Checking sector timeseries: culture ## Checking sector timeseries: digital ## Checking sector timeseries: gambling ## Checking sector timeseries: sport ## Checking sector timeseries: telecoms ## Checking sector timeseries: tourism ## Checking sector timeseries: UK ## ...passed ## Checking for outliers on a row by row basis using mahalanobis distance... ## Checking sector timeseries: all_dcms ## Checking sector timeseries: creative ## Checking sector timeseries: culture ## Checking sector timeseries: digital ## Checking sector timeseries: gambling ## Checking sector timeseries: sport ## Checking sector timeseries: telecoms ## Checking sector timeseries: tourism ## Checking sector timeseries: UK ## ...passed This is a semi-automated process so the user should check the Checks and ensure they meet their usual checks that would be conducted manually. If a new check or test becomes necessary then it should be implemented by changing the code. body(year_sector_data) ## { ## message(&quot;Initiating year_sector_data class.\\n\\n\\nExpects a data.frame with three columns: sector, year, and measure, where\\nmeasure is one of GVA, exports, or enterprises. The data.frame should include\\nhistorical data, which is used for checks on the quality of this year&#39;s data,\\nand for producing tables and plots. More information on the format expected by\\nthis class is given by ?year_sector_data().&quot;) ## message(&quot;\\n*** Running integrity checks on input dataframe (x):&quot;) ## message(&quot;\\nChecking input is properly formatted...&quot;) ## message(&quot;Checking x is a data.frame...&quot;) ## if (!is.data.frame(x)) ## stop(&quot;x must be a data.frame&quot;) ## message(&quot;Checking x has correct columns...&quot;) ## if (length(colnames(x)) != 3) ## stop(&quot;x must have three columns: sector, year, and one of GVA, export, or x&quot;) ## message(&quot;Checking x contains a year column...&quot;) ## if (!&quot;year&quot; %in% colnames(x)) ## stop(&quot;x must contain year column&quot;) ## message(&quot;Checking x contains a sector column...&quot;) ## if (!&quot;sector&quot; %in% colnames(x)) ## stop(&quot;x must contain sector column&quot;) ## message(&quot;Checking x does not contain missing values...&quot;) ## if (anyNA(x)) ## stop(&quot;x cannot contain any missing values&quot;) ## message(&quot;Checking for the correct number of rows...&quot;) ## if (nrow(x) != length(unique(x$sector)) * length(unique(x$year))) { ## warning(&quot;x does not appear to be well formed. nrow(x) should equal\\nlength(unique(x$sector)) * length(unique(x$year)). Check the of x.&quot;) ## } ## message(&quot;...passed&quot;) ## message(&quot;\\n***Running statistical checks on input dataframe (x)...\\n\\n These tests are implemented using the package assertr see:\\n https://cran.r-project.org/web/packages/assertr for more details.&quot;) ## value &lt;- colnames(x)[(!colnames(x) %in% c(&quot;sector&quot;, &quot;year&quot;))] ## message(&quot;Checking years in a sensible range (2000:2020)...&quot;) ## assertr::assert_(x, assertr::in_set(2000:2020), ~year) ## message(&quot;Checking sectors are correct...&quot;) ## sectors_set &lt;- c(creative = &quot;Creative Industries&quot;, culture = &quot;Cultural Sector&quot;, ## digital = &quot;Digital Sector&quot;, gambling = &quot;Gambling&quot;, sport = &quot;Sport&quot;, ## telecoms = &quot;Telecoms&quot;, tourism = &quot;Tourism&quot;, all_dcms = &quot;All DCMS sectors&quot;, ## perc_of_UK = &quot;% of UK GVA&quot;, UK = &quot;UK&quot;) ## assertr::assert_(x, assertr::in_set(names(sectors_set)), ## ~sector, error_fun = raise_issue) ## message(&quot;Checking for outliers (x_i &gt; median(x) + 3 * mad(x)) in each sector timeseries...&quot;) ## series_split &lt;- split(x, x$sector) ## lapply(X = series_split, FUN = function(x) { ## message(&quot;Checking sector timeseries: &quot;, unique(x[[&quot;sector&quot;]])) ## assertr::insist_(x, assertr::within_n_mads(3), lazyeval::interp(~value, ## value = as.name(value)), error_fun = raise_issue) ## }) ## message(&quot;...passed&quot;) ## message(&quot;Checking for outliers on a row by row basis using mahalanobis distance...&quot;) ## lapply(X = series_split, FUN = maha_check) ## message(&quot;...passed&quot;) ## structure(list(df = x, colnames = colnames(x), type = colnames(x)[!colnames(x) %in% ## c(&quot;year&quot;, &quot;sector&quot;)], sector_levels = levels(x$sector), ## sectors_set = sectors_set, years = unique(x$year)), class = &quot;year_sector_data&quot;) ## } The function is structured to tell the user what check is being made and then running that check given the input x. If the input fails a check the function is stopped with a useful diagnostic message for the user. This is achieved using if and the opposite of the desired feature of x. message(&quot;Checking x has correct columns...&quot;) if (length(colnames(x)) != 3) stop(&quot;x must have three columns: sector, year, and one of GVA, export, or x&quot;) For example, if x does not have exactly three columns we stop. 4.3.3.4 Output of this function The output object is different to the input as expected, yet it does contain the initial data. identical(gva$df, x) ## [1] TRUE The rest of the list contains other details that could be changed at a later date if required, demonstrating defensive programming. For example, the sectors that are of interest to DCMS have changed and may change again. ?year_sector_data Let’s take a closer look at this function using the help and other standard function exploration functions. The help says it produces a custom class of object with five slots. isS4(gva) ## [1] FALSE class(gva) ## [1] &quot;year_sector_data&quot; It’s not actually an S4 object, by slots the author means a list of objects. This approach is sensible and easy to work with, as most users are familiar with S3. 4.3.3.5 The input The input, which is likely a bunch of not tidy or messy spreadsheets needs to be wrangled and aggregated (if necessary) for input into the functions prefixed by figure. dplyr::glimpse(GVA_by_sector_2016) ## Observations: 54 ## Variables: 3 ## $ sector &lt;fctr&gt; creative, culture, digital, gambling, sport, telecoms,... ## $ year &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2... ## $ GVA &lt;dbl&gt; 65188, 20291, 97303, 8407, 7016, 24738, 49150, 69398, 2... 4.3.3.6 The R output We build our functions to use the same simple, tidy, data. - Matt Upson With the data in the appropriate form to be received as an argument or input for the figure family of functions, we can proceed to plot. figure3.1(x = gva) Again we can look at the details of the plot. We could change the body of the function to affect change to the default plot or we can pass additional ggplot arguments to it. Reading the code we see it filters the data, makes the variables it needs, refactors the sector variable and then plots it. body(figure3.1) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~sector != &quot;UK&quot;) ## x &lt;- dplyr::mutate_(x, year = ~factor(year, levels = c(2016:2010))) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)]), ## levels = rev(as.character(unname(sectors_set[levels(x$sector)])))) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~GVA, x = ~sector, ## fill = ~year) + ggplot2::geom_bar(colour = &quot;slategray&quot;, ## position = &quot;dodge&quot;, stat = &quot;identity&quot;) + ggplot2::coord_flip() + ## govstyle::theme_gov(base_colour = &quot;black&quot;) + ggplot2::scale_fill_brewer(palette = &quot;Blues&quot;) + ## ggplot2::ylab(&quot;Gross Value Added (£bn)&quot;) + ggplot2::theme(legend.position = &quot;right&quot;, ## legend.key = ggplot2::element_blank()) + ggplot2::scale_y_continuous(labels = scales::comma) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.1():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.1():&quot;, e) ## }, finally = { ## }) ## } We can inspect and change an argument if we feel inclined or if a new colour scheme becomes preferred for example. However, there is no ... in the body of the function itself so where does this argument get passed to? This all looks straight forward and we can inspect the other functions for generating the figures or plot output. body(figure3.2) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~sector %in% c(&quot;UK&quot;, &quot;all_dcms&quot;)) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)])) ## x &lt;- dplyr::group_by_(x, ~sector) ## x &lt;- dplyr::mutate_(x, index = ~max(ifelse(year == 2010, ## GVA, 0)), indexGVA = ~GVA/index * 100) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~indexGVA, ## x = ~year, colour = ~sector, linetype = ~sector) + ## ggplot2::geom_path(size = 1.5) + govstyle::theme_gov(base_colour = &quot;black&quot;) + ## ggplot2::scale_colour_manual(values = unname(govstyle::gov_cols[c(&quot;red&quot;, ## &quot;purple&quot;)])) + ggplot2::ylab(&quot;GVA Index: 2010=100&quot;) + ## ggplot2::theme(legend.position = &quot;bottom&quot;, legend.key = ggplot2::element_blank()) + ## ggplot2::ylim(c(80, 130)) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.2():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.2():&quot;, e) ## }, finally = { ## }) ## } body(figure3.3) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~!sector %in% c(&quot;UK&quot;, &quot;all_dcms&quot;)) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)])) ## x &lt;- dplyr::group_by_(x, ~sector) ## x &lt;- dplyr::mutate_(x, index = ~max(ifelse(year == 2010, ## GVA, 0)), indexGVA = ~GVA/index * 100) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~indexGVA, ## x = ~year, colour = ~sector, linetype = ~sector) + ## ggplot2::geom_path(size = 1.5) + govstyle::theme_gov(base_colour = &quot;black&quot;) + ## ggplot2::scale_colour_brewer(palette = &quot;Set1&quot;) + ## ggplot2::ylab(&quot;GVA Index: 2010=100&quot;) + ggplot2::theme(legend.position = &quot;right&quot;, ## legend.key = ggplot2::element_blank()) + ggplot2::ylim(c(80, ## 150)) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.2():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.2():&quot;, e) ## }, finally = { ## }) ## } 4.3.3.7 Error handling A point of interest in the code with which some users may be unfamiliar is tryCatch which is a function that allows the function to catch conditions such as warnings, errors and messages. We see this towards the end of the function where if either of these conditions are produced then an informative message is produced (in that it tells you in what function there was a problem). The structure here is simple and could be copied and pasted for use in automating other figures of other chapters or statistical reports. For a comprehensive introduction see Hadley’s Chapter. 4.4 Chapter plenary We have explored the eesectors package from the perspective of someone wishing to develop our own semi-automated chapter production through the development of a package in R. This package provides a useful tempplate where one could copy the foundations of the package and workflow. "],
["open.html", "Chapter 5 Open Source rather than proprietary", " Chapter 5 Open Source rather than proprietary Open source languages such as Python and R are increasing in popularity across government. One advantage of using these tools is that we can reduce the number of steps where the data needs to be moved from one program (or format) into another. This is in line with the principle of reproducibility given in guidance on producing quality analysis for government (the AQUA book), as the entire process can be represented as a single step in code, greatly reducing the likelihood of manual transcription errors. Chapter 3 discussed why spreadsheets are dangerous. Moving away from proprietary software, towards Open Source, may also have the additional benefit of being more compatable with tried and tested software development tools and techniques (as well as the obvious no longer needing to pay for it). In GDS’s project with DCMS described in Chapter 4, we decided to use the R language, however we could equally have chosen to use Python or another language entirely: the techniques we outline in this book are language agnostic. "],
["vs.html", "Chapter 6 Version Control 6.1 Introduction 6.2 Useful resources 6.3 Typical workflow 6.4 Branch naming etiquette 6.5 Watermarking", " Chapter 6 Version Control 6.1 Introduction Few software engineers would embark on a new project without using some sort of version control software. Version control software allows us to track the three Ws: Who made Which change, and Why?. Tools like git can be used to track files of any type, but are particularly useful for code in text files for example R or Python code. Whilst git can be used locally on a single machine, or many networked machines, git can also be hooked up to free cloud services such as GitHub, GitLab, or Bitbucket(https://bitbucket.org/). Each of these services provides hosting for your version control repository, and makes the code open and easy to share. The entire project we are working on with DCMS can be seen on GitHub. Obviously this won’t be appropriate for all Government projects (and solutions do exist to allow these services to be run within secure systems), but in our work with DCMS, we were able to publish all of our code openly. You can use our code to run an example based on the 2016 publication, but producing the entire publication from end to end would require access to data which is not published openly. Below is a screenshot from the commit history showing collaboration between data scientists in DCMS and GDS. The full page can be seen on GitHub. Using a service like GitHub allows us to formalise the system of quality assurance (QA) in an auditable way. We can configure GitHub to require a code review by another person before the update to the code (this is called a pull request) is accepted into the main workstream of the project. You can see this in the screenshot below which relates to a pull request which fixed a minor bug in the prototype. The work to fix it was done by a data scientist at DCMS, and reviewed by a data scientist from GDS. The open nature of the good is great for transparency and facilitates review. The entire community can contribute to helping QA your code and identify issues or bugs. If you are lucky, they will not only report the bug / issue, but may also offer a fix for your code in the form of a pull request. 6.2 Useful resources 6.2.1 Graphical user interface focus A useful book on Git and Github that should cover all your needs for those who are uncomfortable working in a command line interface. This will cover most of your Git and Github workflow needs for collaborating in a team. However, we recommend putting the effort into learning git without a GUI, so that you benefit from the full funcitonality on offer. 6.2.2 Git and RStudio You can also use git and Github within R Studio. 6.2.3 Command line focus However, the terminal isn’t that scary really and we recommend using it from the outset. Here’s a video tutorial that provides a good introduction and does not expect any experience of using the Unix shell (the terminal or command line). For a comprehensive tome try the Pro Git book. 6.3 Typical workflow When you first start using git it can be difficult to remember all the commonly used commands (you might find it useful to keep a list of them in a text editor). We give a simple workflow here (assuming you are collaborating on Github with a small team and have set up a repo called my_repo with the origin and remote set (try to avoid hyphens in names)). Remember to remove the comments (the #) when copying and pasting into the terminal. You will also need to give your new feature branch a good name. Open your terminal (command line tool). Navigate to my_repo using cd. Check you are up-to-date: # git checkout master # git pull Create your new feature branch to work on and get to work (track changes by adding and comitting locally as usual): # git checkout -b feature/post_name Squash your commits if appropriate, then push your new branch to Github. You will want to squash all commits together associated with one discrete piece of work (e.g. coding one function). # git push origin feature/post_name -u On Github create a pull request and ask a colleague to review your changes before merging with the master branch (you can assign a reviewer in the PR page on Github). If accepted (and it passes all necessary checks) you’re new feature will have been merged on Github. Fetch these changes locally. # git checkout master # git pull You have a new master on Github. Pull it to your local machine and the development cycle starts again! CAVEAT: this workflow is not appropriate for large open collaborations, where fork and pull is preferred. 6.4 Branch naming etiquette Generally I will start a new branch to either add a new feature git branch feature/cool_new_feature such as for testing or adding a new function git branch feature/sen_function_name. Or if fixing a bug or problem. We should be writing these on the Github issues page for the package. We can then title our branches to tackle specific issues git branch fix/issue_number. It’s good to push your branches to Github if your working on it prior to it being finished so we all know what everyone is working on. You can simply put [WIP] (Work in progress) in the title of the PR on Github to let people know its not ready for review yet. 6.5 Watermarking Imagine if someone asks you to reproduce some historic analysis further down the line. This will be easy if you’ve used git as long as you know which version of your code was used to produce the report (packrat also facilitates this). You can then load that version of the code to repeat the analysis and reproduce the report. As an additional measure, or if you find versioning intimidating, you could watermark your report by citing the git commit used to generate it, as demonstrated below and in the stackoverflow answer by Wander Nauta, 2015. print(system(&quot;git rev-parse --short HEAD&quot;, intern = TRUE)) ## [1] &quot;07a05c7&quot; This commit hash can be used to “revert” back to the code at the time the report was produced, fool around and reproduce the original report. You also have the flexibility to do other things which are explored in this Stack Overflow answer. This feature of version control is what makes our analytical pipelines reproducible. "],
["package.html", "Chapter 7 Packaging Code 7.1 Essential reading 7.2 Development best practices for your package", " Chapter 7 Packaging Code A package enshrines all the business knowledge used to create a corpus of work in one place; including the code and its relevant documentation. One of the difficulties that can arise in the more manual methods of statistics production is that we have many different files relating to many different stages of the process, each of which needs to be documented, and kept up to date. Part of the heavy lifting can be done here with version control as described in Chapter 6, but we can go a step further: we can create a package of code. As Hadley Wickham (author of a number of essential packages for package development) puts it for R: Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. - Hadley Wickham Since it is a matter of statute that we produce our statistical publications, it is essential that our publications are as reproducible as possible. Packaging up the code can also help with institutional knowledge transfer. This was exemplified in Chapter 4 where we explored help files associated with code using the R ? function. library(eesectors) ?clean_sic() Linking the documentation to the code makes everything much easier to understand, and can help to minimising the time taken to bring new team members up to speed. This all meets the requirements of the AQUA book in that all assumptions and constraints can be described in the package documentation asssociated tied to the relevant code. 7.1 Essential reading Hadley Wickham’s R Packages book is an excellent and comprehensive introduction to developing your own package in R. It encourages you to start with the basics and improve over time; good advice. 7.2 Development best practices for your package 7.2.1 Licensing your code Developing your code as an R package will require you to specify a license for your code in the DESCRIPTION file (for example the eesectors package uses the GPL-3 license). We quote the GDS Service Manual by encouraging the use of an Open Source Initiative compatible licence. For example, GDS uses the MIT licence. It is also of note that all code produced by civil servants is automatically covered by Crown Copyright. 7.2.2 Acting as the custodian for your code When you make your code open, you should: use Semantic Versioning to make it clear when you release an update to your code be clear about how you’ll communicate with users of your code, for example on support channels and email lists Encouraging contributions from people who use your code can help make your code more robust, as people will spot bugs and suggest new features. If you would like to encourage contributions, you can create a CONTRIBUTING.md file on Github, like we demonstrate for this book. "],
["test.html", "Chapter 8 Unit test 8.1 Further Reading", " Chapter 8 Unit test Testing is a vital part of package development. It ensures that your code does what you want it to do. We can facilitate testing and Quality Assurance (QA) by building packages with generalisable LEGO-like functions. In procedural programming, code is designed to be reused again and again with different inputs, making for simpler code that is easier to understand and audit. It means we can easily build tests to ensure the code continues to work as expected when we make changes to it. Since each function or group of functions (unit) is generic, it can be tested with a generic example, so that we know that our unit of code works as expected. If we discover cases where our units do not do perform as expected, we can codify these cases into new tests and work to fix the problem until the test passes. We may even go a step further and adopt the practice of test driven development: starting each unit of code with a test which fails, until we write code which can pass the test. Test-driven development is a useful paradigm for developing your code in a thoroughly QA’d fashion. 8.1 Further Reading Much of the heavy lifting for this kind of testing can be done in a unit testing framework, for example testthat for R, or nosetools in Python. For how to test in R read the R package, Testing Chapter "],
["travis.html", "Chapter 9 Automated Testing", " Chapter 9 Automated Testing Once we are in the habit of packaging our code and writing unit tests, we can start to use free online tools such as Travis CI, Jenkins, or Appveyor to automatically test that our package of code builds, and that the tests we have written, pass. These tools integrate with GitHub, so we can easily see when an update to the code has failed one of our tests. The green ticks in the box below show that our tests have passed for the given pull request. We can also look at our test history on Travis CI, as in the screenshot below. From this we can see that our main workstream - the default branch - has been tested 619 times to date, the last of which was one day ago, and the previous five tests have all passed without problems. "],
["code-cover.html", "Chapter 10 Code coverage", " Chapter 10 Code coverage An additional set of tools we can start to use once we begin writing our own tests is code coverage tools, for instance codecov.io, or coveralls.io. These tools are able to analyse the code we have written via hosting services like GitHub, and provide a line by line breakdown of which lines are tested, and which are not. For example, in the lines below from the file year_sector_table.R, we can see that lines 112-115 and 117-120 are not explicitly tested. In this case, we probably don’t need to worry very much, but on other occasions this might prompt us to write more tests. "],
["dep.html", "Chapter 11 Dependency management", " Chapter 11 Dependency management If you want your code to be reproducible in the long-run (i.e. so you can come back to run it next month or next year), you’ll need to track the versions of the packages that your code uses. A rigorous approach is to use packrat, http://rstudio.github.io/packrat/, which store packages in your project directory, or checkpoint, https://github.com/RevolutionAnalytics/checkpoint, which will reinstall packages available on a specified date. A quick and dirty hack is to include a chunk that runs sessionInfo() — that won’t you let easily recreate your packages as they are today, but at least you’ll know what they were. - Hadley Wickham, R for Data Science One of the problems with working with open source software is that it is quite easy to fall into a trap called ‘dependency hell’. Essentially, this occurs when the software we write depends on open source packages, which depend on other open source packages, which can depend on other packages, and on, and on. All these packages may be written by many different people, and are updated at vastly different timescales. If we fail to take account of this, then we are likely to fail at the first hurdle of reproducibility, and we may find that in a year’s time we are no longer able to reproduce the work that we previously did - or at least not without a lot of trouble. There are several ways we might get on top of this problem, but in this project we opted for using packrat, which creates a cache of all the R packages used in the project which is then version controlled on GitHub. Matt Upson has blogged about this previously in the context of writing academic works. "],
["qa-data.html", "Chapter 12 Quality Assurance of the pipeline 12.1 Testing the input data 12.2 Murphy’s Law and errors in your pipeline 12.3 Error handling 12.4 Error logging 12.5 Proof calculation 12.6 When should one stop testing software?", " Chapter 12 Quality Assurance of the pipeline All the testing we have described so far is to do with the code, and ensuring that the code does what we expect it to, but because we have written an R package, it’s also very easy for us to institute tests for the consistency of the data at the time the data is loaded. We may also wish to employ defensive programming against potential errors and consider how we might want to flag these for the user and / or how our pipeline might recover from such errors. 12.1 Testing the input data If our RAP were a sausage factory, the data would be the input meat. Given we do not own the input data nor are we responsible for its preparation we should plan for how we can protect our pipeline against a change in input data format or any anomalous data therein. The list of tests that we might want to run is endless, and the scope of tests very much be dictated by the team which has the expert knowledge of the data. In the eesectors package we implemented two very simple checks, but these could very easily be expanded. The simplest of these is a simple test for outliers: since the data for the economic estimates is longitudinal, i.e. stretching back several years; we are able to look at the most recent values in comparison to the values from previous years. If the latest values lie within a threshold determined statistically from the other values then the data passes, if not a warning is raised. These kinds of automated tests are repeated every time the data are loaded, reducing the burden of QA, and the scope for human error, freeing up statistician time for identifying more subtle data quality issues which might otherwise go unnoticed. 12.2 Murphy’s Law and errors in your pipeline Paraphrasing from Advanced R by Hadley Wickham: If something can go wrong, it will: the format of the spreadsheet you normally receive your raw data in changes, the server you’re talking to may be down, your Wi-Fi drops. Any such problem may stop a piece of your pipeline (code) from doing what it is intended to do. This is not a bug; the problem did not originate from within the code itself. However, if the pipeline downstream is dependent on this code acting as intended then you have a problem, you need to deal with the error somehow. Errors aren’t caused by bugs per se, but neglecting to handle an error appropriately is a bug. 12.3 Error handling Not all problems are unexpected. For example, an input data file may be missing. We can often anticipate some of these likely problems by thinking about our users and how the code we might be implemented or misunderstood. If something goes wrong, we want the user to know about it. We can communicate to the user using a variety of conditions; messages, warnings and errors. If we want to let the user know about something fairly inoccuous or keep them informed we can use the message() function. Sometimes we might want to draw the users attention to something that might be problematic without stopping the code from running, using warning(). If there’s no way for the code to execute then a fatal error may be preferred using stop(). As an example we look to code from the RAP eesectors package produced in collaboration with DCMS. Specifically, we look at a snippet of code from the year_sector_data function. message(&#39;Checking x does not contain missing values...&#39;) if (anyNA(x)) stop(&quot;x cannot contain any missing values&quot;) message(&#39;Checking for the correct number of rows...&#39;) if (nrow(x) != length(unique(x$sector)) * length(unique(x$year))) { warning(&quot;x does not appear to be well formed. nrow(x) should equal length(unique(x$sector)) * length(unique(x$year)). Check the of x.&quot;) } message(&#39;...passed&#39;) We’ll work our way through the code above, step by step. The first line informs the user of the quality assurance that is being automatically conducted, i.e. that no data is missing (NA). The second line uses the logical if statement to assess this on x and if it were true, we use stop to produce a fatal error with a descriptive message, as below. x &lt;- c(&quot;culture&quot;, &quot;sport&quot;, NA) message(&#39;Checking x does not contain missing values...&#39;) ## Checking x does not contain missing values... if (anyNA(x)) stop(&quot;x cannot contain any missing values&quot;) ## Error in eval(expr, envir, enclos): x cannot contain any missing values The third line checks that the data has data for each sector for each year (indirectly). If not it throws up a warning, as this could be for a valid reason (e.g. a change of name of a factor level), but it’s better to let the user know rather than let it quietly pass. Thus all the expert domain knowledge can be incorporated into the code through condition handling, providing transparent quality assurance. These informative messages are useful but when used in conjuction with tryCatch, we can implement our own custom responses to a message, warning or error (this is explained in the relevant Advanced R Chapter). We demonstrate a simplified example from the eesectors package where an informative message is provided. This is achieved by wrapping the main body of the function within the tryCatch function. # Define as a method figure3.1 &lt;- function(x, ...) { out &lt;- tryCatch( expr = { p &lt;- plot(x, y) return(p) }, warning = function() { w &lt;- warnings() warning(&#39;Warning produced running figure3.1():&#39;, w) }, error = function(e) { stop(&#39;Error produced running figure3.1():&#39;, e) }, finally = {} ) } The body of a tryCatch() must be a single expression; using { we combine several expressions into a single form. This a simple addition to our function but it’s powerful in that it provides the user with more information for anticipated problems. By wrapping the code in the tryCatch function we ensure that it gets evaluated, whereas normally an error would cause the evaluation of the code to stop. Instead we get the error and the evaluation of the next line of code. try(print(this_object_does_not_exist)); print(&quot;What happens if this is not wrapped in try?&quot;) ## [1] &quot;What happens if this is not wrapped in try?&quot; 12.4 Error logging We are increasingly using R and software packages like our RAP in “operational” settings that require robust error handling and logging. In this section we describe a quick-and-dirty way to get python style multi-level log files by wrapping the futile.logger package inspired by this blog post. 12.4.1 Pipeline pitfalls Our real world scenario involves an R package that processes raw data (typically from a spreadsheet or SQL table) that is updated periodically. The raw data can come from various different sources, set up by different agencies and usually manually procured or copy and pasted together prioritising human readability over machine readability. Data could be missing or in a different layout from that which we are use to, for a variety of reasons, before it even gets to us. And then our R functions within our package may extract this data and perform various checks on it. From there a user (analyst / statistician) may put together a statistical report using Rmarkdown ultimately resulting in a html document. This pipeline has lots of steps and potential to encounter problems throughout. As we develop our RAP for our intended bespoke problem and start to use it in an operational setting, we must ensure that in this chaotic environment we protect ourselves against things going wrong without our realising. One of the methods for working with chaotic situations in operational software is to have lots and Lots and LOTS of logging. We take our inspirration from Python, which has the brilliant “logging” module that allows us to quickly set up separate output files for log statements at different levels. This is important as we have different users who may have different needs from the log files. For example, the data scientist / analyst who did the programming for the RAP package may wish to debug the code on logging an error whereas a statistician may prefer to be notified only when an ERROR or something FATAL occurred. 12.4.2 Error logging using futile.logger Fortunately there’s a package available on CRAN that makes this process easy in R called futile.logger. There are a few concepts that it’s helpful to be familiar with before proceeding which are introduced in this blog post by the package author. One approach is to replace tryCatch with the ftry function with the finally. This function integrates futile.logger with the error and warning system so problems can be caught both in the standard R warning system, while also being emitted via futile.logger. We think about how to adapt our earlier code to this function. The primary use case for futile.logger is to write out log messages. There are log writers associated with all the predefined log levels: TRACE, DEBUG, INFO, WARN, ERROR, FATAL. Log messages will only be written if the log level is equal to or more urgent than the current threshold. By default the ROOT logger is set to INFO but this can be adjusted by the user facilitating customisation of the error logging to meet the needs of the current user (by using the flog.threshold function). We demonstrate this hierarchy below by evaluating this code. # library(futile.logger) futile.logger::flog.debug(&quot;This won&#39;t print&quot;) futile.logger::flog.info(&quot;But this %s&quot;, &#39;will&#39;) ## INFO [2019-01-09 10:25:43] But this will futile.logger::flog.warn(&quot;As will %s&quot;, &#39;this&#39;) ## WARN [2019-01-09 10:25:43] As will this x &lt;- c(&quot;culture&quot;, &quot;sport&quot;, NA) message(&#39;Checking x does not contain missing values...&#39;) if (anyNA(x)) stop(&quot;x cannot contain any missing values&quot;) We start by re-writing the above code using the futile.logger high level interface. As the default setting is at the INFO log level we can use flog.trace to hide most of the checks and messages from a typical user. # Data from raw has an error x &lt;- c(&quot;culture&quot;, &quot;sport&quot;, NA) ### Non-urgent log level futile.logger::flog.trace(&quot;Checking x does not contain missing values...&quot;) ### Urgent log level, use capture to print out data structure if (anyNA(x)) { futile.logger::flog.error(&quot;x cannot contain any missing values.&quot;, x, capture = TRUE) } ## ERROR [2019-01-09 10:25:43] x cannot contain any missing values. ## ## [1] &quot;culture&quot; &quot;sport&quot; NA futile.logger::flog.info(&quot;Finished checks.&quot;) ## INFO [2019-01-09 10:25:43] Finished checks. The above example can help the user identify where the pipeline is going wrong by logging the error and capturing the object x where the data is missing. This allows us to more quickly track down what’s going wrong. 12.4.3 Logging to file At the moment we default to writing our log to the console. We could write to a file if interested using the appender family of functions. # Print log messages to the console futile.logger::appender.console() # Write log messages to a file futile.logger::appender.file(&quot;rap_companion.log&quot;) # Write log messages to console and a file futile.logger::appender.tee(&quot;rap_companion.log&quot;) 12.5 Proof calculation Regardless of how the results are published or the methodology used, the results need to be checked for correctness. Here we explore how we can use statistics to help us validate the correctness of results in a RAP. The scientific method of choice to address validity is peer review. This can go as far as having the reviewer implement the analysis as a completely separate and independent process in order to check that results agree. Such a co-pilot approach fits nicely to the fact that real-life statistical analysis rarely is a one-person activity anymore. In practice, there might neither be a need nor the resources to rebuild entire analyses, but critical parts need to be double-checked. There a variety of appraoches you could try that will suit different problems. Pair programming is one technique from the agile programming world to accomodate this. Single programmers coding independently and then comparing results. Peer review of code and tests throughout the development process using Github. In our RAP projects to date we have opted for the third choice, as often our aim is to build programming capability as well as correct and reproducible results through code. We also use unit tests to check the critical areas of code by providing an expectation. However, unit tests are more useful for detecting errors with a code during development, as they are a manifestation of our expert domain knowledge. They are only as comprehensive as the work invested in writing them, conversely one does not need infinite tests. If you are interested in taking these ideas further and using statistics to help you estimate the number of wrong results in your report as part of your QA process, then read this blog. 12.6 When should one stop testing software? Imagine that a team of developers of a new RAP R package needs to structure a test plan before the publication of their report. There is an (unknown) number of bugs in the package. The team starts their testing at time zero and subsequently find an increasing number of bugs as the test period passes by. The figure below shows such a testing process mimicking the example of Dalal and Mallows (1988) from the testing of a large software system at a telecommunications research company. We see that the number of bugs appears to level off. The question is now how long should we continue testing before releasing? For a discussion of this problem, see this blog, from which we have paraphrased. "],
["pub.html", "Chapter 13 Producing the publication 13.1 R Markdown overview 13.2 R Markdown basics 13.3 Text formatting with Markdown 13.4 Code chunks 13.5 YAML header 13.6 Further Reading", " Chapter 13 Producing the publication R Markdown provides an unified authoring framework for data science, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. - Hadley Wikcham, R for Data Science Everything I have talked about so far is to do with the production of the statistics themselves, not preparation of the final publication, but there are tools that can help with this too. In our project with DCMS we plan to use Rmarkdown (a flavour of markdown) to incorporate the R code into the same document as the text of the publication. Working in this way means that we can do all of the operations in a single file, so we have no problems with ensuring that our tables or figures are synced with the latest version of the text: everything can be produced in a single file. We can even produce templates with boilerplate text like: ‘this measure increased by X%’, and then automatically populate the X with the correct values when we run the code. 13.1 R Markdown overview Copied and paraphrased from Hadley Wickham’s R for Data Science: R Markdown provides an unified authoring framework for analytical reporting, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. R Markdown files as a data product of RAP are designed to be used: For communicating to decision makers or users, who want to focus on the conclusions, not the code behind the analysis. R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead you can rely on the Help within RStudio: R Markdown Cheat Sheet: Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet, R Markdown Reference Guide: Help &gt; Cheatsheets &gt; R Markdown Reference Guide. Both cheatsheets are also available at http://rstudio.com/cheatsheets. 13.1.1 Prerequisites You need the rmarkdown package, but you don’t need to explicitly install it or load it, as RStudio automatically does both when needed. 13.2 R Markdown basics This file itself is an R Markdown file, a plain text file that has the extension .Rmd. Here’s a screenshot of a R Markdown file: It contains three important types of content: An (optional) YAML header surrounded by ---s (lines 1-6). Chunks of R code surrounded by ``` (lines 22-27 and 29-31). Text mixed with simple text formatting like # heading and _italics_ (lines 7-20). 13.3 Text formatting with Markdown Prose in .Rmd files is written in Markdown, a lightweight set of conventions for formatting plain text files. Markdown is designed to be easy to read and easy to write. It is also very easy to learn thus we leave that to the reader to learn themselves through practise. 13.4 Code chunks To run code inside an R Markdown document, you need to insert a chunk. There are three ways to do so: The keyboard shortcut Cmd/Ctrl + Alt + I The “Insert” button icon in the editor toolbar. By manually typing the chunk delimiters ```{r} and ```. Obviously, Hadley Wickham recommends you learn the keyboard shortcut. It will save you a lot of time in the long run! You can continue to run the code using the keyboard shortcut that by now you know and love: Cmd/Ctrl + Enter. However, chunks get a new keyboard shortcut: Cmd/Ctrl + Shift + Enter, which runs all the code in the chunk (Cmd/Ctrl + Shift + N, runs the next chunk). Think of a chunk like a function. A chunk should be relatively self-contained, and focussed around a single task. 13.4.1 Chunking code in RAP Ask your users what they might prefer; all the code in one chunk at the start, specifying all the variables needed for the rest of the document, to keep the code “out of the way”, or each code chunk occuring adjacent to the relevant statistic, figure or table generated. 13.4.2 Chunk name Chunks can be given an optional name: ```{r by-name}. This has three advantages: You can more easily navigate to specific chunks using the drop-down code navigator in the bottom-left of the script editor: Graphics produced by the chunks will have useful names that make them easier to use elsewhere. You can set up networks of cached chunks to avoid re-performing expensive computations on every run. There is one chunk name that imbues special behaviour: setup. When you’re in a notebook mode, the chunk named setup will be run automatically once, before any other code is run. This can be used to set defaut behaviour for all of your chunks as well as a few other special things. 13.4.3 Chunk options Chunk output can be customised with options, arguments supplied to chunk header. Knitr provides almost 60 options that you can use to customize your code chunks. Here we’ll cover the most important chunk options that you’ll use frequently. You can see the full list at http://yihui.name/knitr/options/. The most important set of options controls if your code block is executed and what results are inserted in the finished report: eval = FALSE prevents code from being evaluated. (And obviously if the code is not run, no results will be generated). This is useful for displaying example code, or for disabling a large block of code without commenting each line. include = FALSE runs the code, but doesn’t show the code or results in the final document. Use this for setup code that you don’t want cluttering your report. echo = FALSE prevents code, but not the results from appearing in the finished file. Use this when writing reports aimed at people who don’t want to see the underlying R code. message = FALSE or warning = FALSE prevents messages or warnings from appearing in the finished file. results = 'hide' hides printed output; fig.show = 'hide' hides plots. error = TRUE causes the render to continue even if code returns an error. This is rarely something you’ll want to include in the final version of your report, but can be very useful if you need to debug exactly what is going on inside your .Rmd. It’s also useful if you’re teaching R and want to deliberately include an error. The default, error = FALSE causes knitting to fail if there is a single error in the document. The following table summarises which types of output each option supressess: Option Run code Show code Output Plots Messages Warnings eval = FALSE - - - - - include = FALSE - - - - - echo = FALSE - results = &quot;hide&quot; - fig.show = &quot;hide&quot; - message = FALSE - warning = FALSE - 13.4.4 Global options As you work more with knitr, you will discover that some of the default chunk options don’t fit your needs and you want to change them. You can do this by calling knitr::opts_chunk$set() in a code chunk. For example, when business reports and statistical publications try: knitr::opts_chunk$set( echo = FALSE ) That will hide the code by default, so only showing the chunks you deliberately choose to show (with echo = TRUE). You might consider setting message = FALSE and warning = FALSE, but that would make it harder to debug problems because you wouldn’t see any messages in the final document. 13.4.5 Inline code Often a report might have statistics within a sentence of prose. There is a way to embed R code into prose, with inline code. This can be very useful if you mention properties of your data in the text. You might want to write functions that prettify to create the desired kind of output (i.e. rounding to two digits and a % suffixed). Inline output is indistinguishable from the surrounding text. Inline expressions do not take knitr options. 13.5 YAML header You can control many other “whole document” settings by tweaking the parameters of the YAML header. You might wonder what YAML stands for: it’s “yet another markup language”, which is designed for representing hierarchical data in a way that’s easy for humans to read and write. R Markdown uses it to control many details of the output. 13.5.1 Output style You can output in a variety of file formats and customise the appearance of your output. 13.5.2 Parameters R Markdown documents can include one or more parameters whose values can be set when you render the report. Parameters are useful when you want to re-render the same report with distinct values for various key inputs. For example, you might be producing sales reports per branch, exam results by student, or demographic summaries by country. To declare one or more parameters, use the params field. In RStudio, you can click the “Knit with Parameters” option in the Knit dropdown menu to set parameters, render, and preview the report in a single user friendly step. You can customise the dialog by setting other options in the header. See http://rmarkdown.rstudio.com/developer_parameterized_reports.html#parameter_user_interfaces for more details. 13.6 Further Reading R Markdown is still relatively young, and is still growing rapidly. The best place to stay on top of innovations is the official R Markdown website: http://rmarkdown.rstudio.com and https://rmarkdown.rstudio.com/articles.html. You can write individual chapters using R markdown, as one file per Chapter. Alternatively you can write the whole publication using bookdown. For a basic start in bookdown try this blog post (we wrote this book using this to kick things off). "],
["final.html", "Chapter 14 Final Thoughts 14.1 A Structured RAP Course 14.2 RAP MOOC 14.3 User feedback 14.4 Just the beginning…", " Chapter 14 Final Thoughts It’s called Data Science for a reason, record all the data handling, experimentation and analysis in your lab-notebook. Version control is your digital lab notebook. 14.1 A Structured RAP Course You’re welcome to dip into the previous chapters as and when you need, but you may prefer a more comprehensive grounding in the principles of reproducibility. We provide a sequenced list of lessons here to help you on your journey in becoming a RAP champion. We suggest you work through the list. The links designated as HELP are provided as comprehensive resources if you get stuck, and can be otherwise skipped. The estimated completion time for a resource is given in parentheses. The Unix Shell or terminal Interactive lesson that prepares you for git (3 hours) Version Control with git Quick overview for those without a computing background (30 mins) Interactive lesson (3 hours) HELP: Comprehensive git book git and Github The difference between git and Github (5 mins) ADVANCED: Github workflow written (1 hour) Github workflow visual RStudio, R projects and R fundamentals Interactive lesson (12 hours) Interactive lesson (some overlap with above lesson) (7 hours) HELP: Rmarkdown Passive lesson (3 hours) HELP: Write a book in Rmarkdown using bookdown R packages HELP: Hadley Wickham’s R packages Prevent dependency hell with packrat packrat setup (1 hour) Data concerns Tidy data (1 hour) The minimal tidy data set (1 hour) HELP: thinking about data from spreadsheets HELP: data management with SQL SQL HELP: organising data Unit tests Hadley Wickham’s chapter (2 hours) Automated testing; detects problems you might miss. Using RMD check Getting started with Travis Building an R project with Travis Look at an example RAP .travis.yml Further reading Software carpentry Data carpentry DevOps DataOps Awesome R packages for DevOps R for Data Science Evidence based data analysis It’s also worthwhile looking at other Department’s RAP efforts. For a good open example see DCMS’s eesectors package. 14.2 RAP MOOC To complement this book, one of our RAPpers has developed a Massive Online Open Course to share an approach to learning this technical skill-set. This course is an informal introduction and describes the best practices through the use of screencasts and assignments. It is currently available on Udemy and takes you through the RAP journey using a simple RAP example. 14.3 User feedback The RAP companion is intended to point data scientists in the Civil Service towards the Data Ops toolkit that should be used when attempting to automate some of the boring stuff for their colleagues. Your feedback is welcome and important. It will help us improve the RAP companion through further iterations. You can feedback through completing this Google form which allows us to measure your satisfaction. Or, by raising an issue on the RAP companion repo page. 14.4 Just the beginning… We’ve introduced you to the basics of reproducibility and Data Ops. For further development ideas and inspiration consider the Data Ops manifesto. "]
]
