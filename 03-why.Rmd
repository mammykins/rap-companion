# Why RAP? {#why}

Reproducible Analytical Pipelines require a range of tools and techniques to implement that can be a challenge to overcome; so why bother learning to RAP?

* Does your team spend too much time moving data between various softwares?
* Could you reproduce your most recent publication's stats? How about from five years ago?
* What would your team do with their time if it was freed up from copying and pasting?
* What proportion of spreadsheets contain errors?

## The current statistics production process

Here we use the production of official statistics in Government; altough this process varies widely perhaps it looks something like this?

```{r fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), fig.link='https://gdsdata.blog.gov.uk/2017/03/27/reproducible-analytical-pipeline/'}
knitr::include_graphics('images/messy_pipeline.png', dpi = NA)
```

Broadly speaking, data are extracted from a datastore (whether it is a [data lake](https://en.wikipedia.org/wiki/Data_lake), [database](https://en.wikipedia.org/wiki/Database), spreadsheet, or [flat file](https://en.wikipedia.org/wiki/Flat_file_database)), and are manipulated in a proprietary statistical software package, and possibly in proprietary spreadsheet software. Formatted tables are often then ‘copy and pasted’ into a word processor, before being converted to pdf format, and finally published to [GOV.UK](https://www.gov.uk/). This is quite a simplification, as statistical publications are usually produced by several people, so this process is likely to be happening in parallel many times.

### The problem with this approach

Errors in [spreadsheets](http://faculty.tuck.dartmouth.edu/images/uploads/faculty/serp/Errors.pdf) are common due to [human error](https://arxiv.org/abs/1602.02601)^[Research on spreadsheet errors is substantial, compelling, and unanimous. It has three simple conclusions. The first is that spreadsheet errors are rare on a per-cell basis, but in large programs, at least one incorrect bottom-line value is very likely to be present. The second is that errors are extremely difficult to detect and correct. The third is that spreadsheet developers and corporations are highly overconfident in the accuracy of their spreadsheets.]. We can mitigate these issues by minimising the role humans play in the tasks that they perform poorly^[Tasks that machines excel at]. This will free up human time to focus on complex tasks such as the interpretation of the statistics and communicating the implications of these findings to others.  

Other issues are: 

* Copy and pasting.
* Lack of proper version control.
* Testing and QA normally happens at the end.

A key element in this process is quality assurance (QA). Each publication is meticulously checked to ensure the accuracy of the statistics being produced. This may take place throughout the production process or at the end prior to publication. Traditionally, QA has been a manual process which can take up a significant portion of the overall production time of a publication, as any changes will require the manual process of production to be repeated.

### Desired Reproducible Analytical Pipeline

Our analytical pipeline should have all the hallmarks of good scientific practice - it should be reproducible.  

```{r fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), fig.link='https://gdsdata.blog.gov.uk/2017/03/27/reproducible-analytical-pipeline/'}
knitr::include_graphics('images/rap.png', dpi = NA)
```